# -*- coding: utf-8 -*-
"""Trabalho-MatheusGabardoMessias.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15FOFJ1ClajqPHFe1FJE19YGcrJ82RgGY

# **Trabalho de analise de dados - TÓPICOS DE BIG DATA EM PYTHON.**

### MATHEUS GABARDO MESSIAS -


## Objetivos:


* Dominar o ambiente de desenvolvimento e SVM.
*   Domicar o ambiente de desenvolvimento e o pacote SVC.
    * Google colab.
        É um ambiente de notebooks que permite a criação de documentos interativos que contenham códigos, mas também textos formatados, imagens, gráficos e equações matemáticas. Sendo executado na nuvem do Google não requer configuração.
    * SVM
        É um algoritmo de aprendizado de máquina supervisionado que pode ser usado para desafios de classificação ou regressão. Seu foco maior é no treinamento e classificação de um dataset.
*   Com base no dataset escolhido implementar um modelo para avaliarmos sua acurácia.
    * Dataset escolhido:
    https://www.kaggle.com/datasets/kelvinkelue/credit-card-fraud-prediction
    * Os dados são referentes as transações online feitas em determinada rede nos Estados Unidos de 21/06/2020 há 28/06/2020.
*   Explorar funções de modelagem de dados.
*   Exibir dados em gráficos utilizando alguma biblioteca de exibição de dados.
*   Explorar com percentual de base de testes.
*   Aplicar ao modelo os algoritmos SVC, KNN e Árvore.
*   Avalie a diferença da Acurácia entre os percentuais de testes.
*   Elaborar slides para apresentação contendo:
    *   Dados em gráficos utilizando alguma biblioteca de exibição de dados.
    *   Problematização de acordo com os dados colhidos.
    *   Solução para o problema.
"""

# A mportação das bibliotecas é um passo crucial, assim, adquirirmos "ferramentas" para manipular, exibir e tratar os dados.
import pandas as pd
    #Pandas é uma biblioteca de Python amplamente utilizada para manipulação e análise de dados de forma eficiente.
    #com ela podemos ler e escrever dados em vários formatos, realiza operações de limpeza, transformação e agregação de dados.
import numpy as np
    #Fundamental para computação numérica, fornece estruturas de dados eficientes para armazenar e manipular arrays multidimensionais.
    #inclui funções para operações matemáticas avançadas em arrays, como álgebra linear, transformada de Fourier, geração de números aleatórios, entre outras.
import matplotlib.pyplot as plt
        #É especialmente útil para visualização dos dados complexos, oferecendo controle detalhado sobre a aparência dos gráficos.

# Agora iremos importar o Dataset e salvaremos na variavel "database" para maior clareza e manipulação.

database = pd.read_csv('./data/fraud-cc.csv')

# Iremos utilizar algumas funções da biblioteca Pandas para analisar os dados, sendo elas: .head(), .tail(), .describe(), .info(), dropna()

# Usando a função .head() iremos visualizar os 3 primeiros registros do dataset.

database.head(3)

# Usando a função .tail() iremos visualizar os 3 últimos registros do dataset.
database.tail(3)

# Usando a função .describe() iremos visualizar as estatísticas de cada coluna do dataset, isso pode ser útil para resumir
# rapidamente a natureza dos dados e possivelmente identificar possíveis problemas nos dados, como valores ausentes ou inconsistências.

database.describe()

# No nosso caso conseguimos identificar que uma linha do nosso dataset não possui o valor que nos interessa na coluna is_fraud.

# Agora iremos remover as linhas com dados inconsistentes usando a função .dropna().
#(função utilizada para remover valores nulos ou NaN (Not a Number))

database.dropna(subset=['is_fraud'], inplace=True)

# Agora iremos padronizar a data e criar uma nova coluna com apenas a informação do horario das transações.

# Conversão da coluna de data 'trans_date_trans_time' para tipo datetime (padrão para calculos).
database['trans_date_trans_time'] = pd.to_datetime(database['trans_date_trans_time'], dayfirst=True)

# Extração do horario e salvando em uma nova coluna de nome 'hour'.
database['hour'] = database['trans_date_trans_time'].dt.hour

"""### Com os dados tratados podemos renderizar e analisa-los em forma de gráficos.
Ánalises basicas:
1. Porcentagem de transações fraudulentas.
2. Principais horarios das fraudes.
3. Categorias mais cobiçadas.
4. Estados mais utilizados para fraudes.


"""

# Gráfico com os principais horários das transações fraudulentas.
fraud_hour_counts = database[database['is_fraud'] == 1]['hour'].value_counts().sort_index()
plt.figure(figsize=(6, 3))
fraud_hour_counts.plot(kind='bar', color='red')
plt.title('Principais Horários das Fraudes')
plt.xlabel('Hora do Dia')
plt.ylabel('Número de Fraudes')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

#Array de traduções para "embelezar o grafico"
traducoes = {
    'gas_transport': 'Posto de gasolina',
    'home': 'Mat. de construção',
    'grocery_pos': 'Mercearias',
    'shopping_pos': 'Lojas multicoisas',
    'kids_pets': 'Petshop',
    'shopping_net': 'Compras on-line',
    'food_dining': 'Jantares',
    'personal_care': 'Cuidado pessoal',
    'entertainment': 'Entreterimento',
    'health_fitness': 'Sáude',
    'misc_net': 'Rede diversa',
    'travel': 'Viagens',
}

total_transacoes_fraudulentas = database['is_fraud'].sum() #transações fraudulentas
transacoes_fraudulentas_por_categoria = database[database['is_fraud'] == 1]['category'].value_counts() #transações fraudulentas por categoria
porcentagem_transacoes_fraudulentas_por_categoria = (transacoes_fraudulentas_por_categoria / total_transacoes_fraudulentas) * 100 #porcentagem de transações fraudulentas por categoria
porcentagem_transacoes_fraudulentas_por_categoria = porcentagem_transacoes_fraudulentas_por_categoria.sort_values(ascending=False) # ordenação das categorias por porcentagem


#Gráfico com as categorias mais cobiçadas.
plt.figure(figsize=(8, 4))
porcentagem_transacoes_fraudulentas_por_categoria.plot(kind='bar', color='blue')
plt.title('Porcentagem de Transações Fraudulentas por Categoria')
plt.xlabel('Categoria')
plt.ylabel('Porcentagem de Transações Fraudulentas')
plt.xticks(rotation=30)

#Legenda para os dados (previamente testados preparados)
for i, v in enumerate(porcentagem_transacoes_fraudulentas_por_categoria):
    plt.text(i, v + 0.5, f'{v:.2f}%', ha='center')

plt.gca().set_xticklabels([traducoes.get(label, label) for label in porcentagem_transacoes_fraudulentas_por_categoria.index])

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

fraud_percentage = (database['is_fraud'].sum() / len(database)) * 100 # Porcentagem de transações fraudulentas.
non_fraud_percentage = 100 - fraud_percentage # Porcentagem de transações não fraudulentas.
total_transacoes_nao_fraudulentas = len(database) - database['is_fraud'].sum()


# Configs para o gráfico de pizza.
labels = ['Fraude', 'Não Fraude']
sizes = [fraud_percentage, non_fraud_percentage]
colors = ['red', 'green']

# Gráfico de pizza.
plt.figure(figsize=(6, 4))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)

# Legenda dos valores em baixo do gráfico.
legend_labels = [f'Transações Fraudulentas: {total_transacoes_fraudulentas}', f'Transações Não Fraudulentas: {total_transacoes_nao_fraudulentas}']
plt.legend(legend_labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), shadow=True, ncol=2)

plt.title('Porcentagem de Transações Fraudulentas')
plt.axis('equal')
plt.show()

# Estados que mais tiveram transações fraudulentas.

traducoes_estados = {
    'NY': 'Nova York',
    'AL': 'Utah',
    'FL': 'Flórida',
    'IN': 'Indiana',
    'LA': 'Louisiana',
    'ME': 'Maine',
    'SD': 'Dakota do Sul',
    'WI': 'Wisconsin',
    'TX': 'Texas',
    'OH': 'Ohio'
}
quantidade_fraudes_estado = database[database['is_fraud'] == 1]['state'].value_counts().nlargest(10) #estados que mais tiveram transações fraudulentas.
quantidade_fraudes_estado.index = quantidade_fraudes_estado.index.map(traducoes_estados) # Substituindo siglas dos estados pelos nomes

plt.figure(figsize=(6, 4))
quantidade_fraudes_estado.plot(kind='bar', color='green')
plt.title('Estados que mais tiveram transações fraudulentas')
plt.xlabel('Estado')
plt.ylabel('Número de Fraudes')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""### Com todos os dados analisados e separados agora partiremos para o treinamento dos modelos, a avaliação dos modelos e a comparação dos resultados.
Primeiro iremos retirar as colunas inúteis.
"""

array_colunas_inuteis = ['trans_date_trans_time', 'merch_long', 'merch_lat', 'unix_time', 'trans_num', 'job', 'city_pop', 'long', 'lat', 'zip', 'city', 'gender', 'first', 'last', 'cc_num', 'Unnamed: 0']

database = database.drop(array_colunas_inuteis, axis=1)

database.info()
database.head()

#Tratando os dados ausentes
database.dropna(inplace=True)
#Convertendo dob para datetime
database['dob'] = pd.to_datetime(database['dob'], dayfirst=True)

#Dividindo os dados em conjuntos de treinamento e teste

from sklearn.model_selection import train_test_split

# Definindo os recursos (features) e o alvo (target)
X = database[['amt']]
y = database['is_fraud']

# Dividindo os dados em conjuntos de treinamento e teste (30% e 50%)
X_train_30, X_test_30, y_train_30, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.5, random_state=42)

#treinando o modelo SVC
from sklearn.svm import SVC
svc_model = SVC()
svc_model.fit(X_train_30, y_train_30)


#treinando o modelo KNN
from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier()
knn_model.fit(X_train_30, y_train_30)

#treinando o modelo Arvore
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train_30, y_train_30)

print('Avaliando os modelos 30%')
# Fazendo previsões e calculando a acurácia para o modelo SVC (30%)
svc_accuracy_30 = svc_model.score(X_test_30, y_test_30)
print("Acurácia do modelo SVC (30% de teste):", svc_accuracy_30)
# Fazendo previsões e calculando a acurácia para o modelo KNN (30%)
knn_accuracy_30 = knn_model.score(X_test_30, y_test_30)
print("Acurácia do modelo KNN (30% de teste):", knn_accuracy_30)
# Fazendo previsões e calculando a acurácia para o modelo Árvore de Decisão (30%)
tree_accuracy_30 = tree_model.score(X_test_30, y_test_30)
print("Acurácia do modelo Tree (30% de teste):", tree_accuracy_30)

print('-')
print('Avaliando os modelos 50%')
# Fazendo previsões e calculando a acurácia para o modelo SVC (50%)
svc_accuracy_50 = svc_model.score(X_test_50, y_test_50)
print("Acurácia do modelo SVC (50% de teste):", svc_accuracy_50)
# Fazendo previsões e calculando a acurácia para o modelo KNN (50%)
knn_accuracy_50 = knn_model.score(X_test_50, y_test_50)
print("Acurácia do modelo KNN (50% de teste):", knn_accuracy_50)
# Fazendo previsões e calculando a acurácia para o modelo Árvore de Decisão (50%)
tree_accuracy_50 = tree_model.score(X_test_50, y_test_50)
print("Acurácia do modelo Tree (50% de teste):", tree_accuracy_50)

# Comparando as acurácias dos modelos para os percentuais de testes de 30% e 50%
print("Diferença de acurácia entre 30% e 50% para o modelo SVC:", svc_accuracy_30 - svc_accuracy_50)

# Comparando as acurácias dos modelos para os percentuais de testes de 30% e 50% para KNN
print("Diferença de acurácia entre 30% e 50% para o modelo KNN:", knn_accuracy_30 - knn_accuracy_50)

# Comparando as acurácias dos modelos para os percentuais de testes de 30% e 50% para Árvore de Decisão
print("Diferença de acurácia entre 30% e 50% para o modelo Tree:", tree_accuracy_30 - tree_accuracy_50)